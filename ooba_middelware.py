import requests
from langchain.llms.base import LLM
from typing import Optional, List, Mapping, Any

class oobaLLM(LLM):
    @property
    def _llm_type(self) -> str:
        return "custom"

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        response = requests.post(
            "http://localhost:5000/api/v1/generate",
            json={
                'prompt': prompt,
                'max_new_tokens': 1000,
                'do_sample': False,
                'temperature': 0.7,
                'top_p': 0.1,
                'typical_p': 1,
                'repetition_penalty': 1.18,
                'top_k': 40,
                'min_length': 0,
                'no_repeat_ngram_size': 0,
                'num_beams': 1,
                  'penalty_alpha': 0,
                'length_penalty': 1,
                'early_stopping': True,
                'seed': -1,
                'add_bos_token': True,
                'truncation_length': 2048,
                'ban_eos_token': False,
                'skip_special_tokens': False,
                # 'stopping_strings': ["\n\n", "Observation:"]
                'stopping_strings': []
            }
        )

        response.raise_for_status()

        return response.json()["results"][0]["text"].strip().replace("```", " ")

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """Get the identifying parameters."""
        return {

        }
